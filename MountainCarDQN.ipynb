{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "https://arxiv.org/pdf/1603.00748.pdf\n",
    "\n",
    "https://github.com/BY571/Normalized-Advantage-Function-NAF-/blob/master/NAF.ipynb\n",
    "\n",
    "https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the transition\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# define the replay memory\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"Implements a Replay Memory.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"Initialize a ReplayMemory object.\"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch_size of transitions from the memory.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"The Q-function approximation via a Deep Neural Network.\"\"\"\n",
    "\n",
    "    def __init__(self, observation_size, action_size):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        super().__init__()  \n",
    "\n",
    "        self.layer1 = nn.Linear(observation_size, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128) # batch normalization, see https://arxiv.org/abs/1502.03167, \n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128) # batch normalization\n",
    "\n",
    "        self.V = nn.Linear(128, 1) # value function\n",
    "        self.mu = nn.Linear(128, 1) # mean of the action distribution\n",
    "        self.L = nn.Linear(128, 1) # lower triangular matrix of the cholesky decomposition of the covariance matrix, here just a single value\n",
    "\n",
    "        \n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x, action=None):\n",
    "        \n",
    "\n",
    "        # propagate the input state\n",
    "        x = F.relu(self.layer1(x))\n",
    "        # x = self.bn1(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        # x = self.bn2(x)\n",
    "\n",
    "        # get the estimate of the value of state x\n",
    "        V = self.V(x) \n",
    "\n",
    "        # get the mu parameter of the advantage function\n",
    "        mu = torch.tanh(self.mu(x)) \n",
    "\n",
    "        # get the P parameer of the advantage function\n",
    "        L = torch.tanh(self.L(x))\n",
    "        P = L * L\n",
    "\n",
    "        Q = None\n",
    "        if action is not None:\n",
    "\n",
    "            # assemble the advantage function\n",
    "            A = -0.5 * (action - mu) * (action - mu) * P \n",
    "\n",
    "            # calculate the q value for the state action pair\n",
    "            Q = V + A\n",
    "\n",
    "        return Q, V, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "# LR = 1e-3 # commented so that the parameter can be controlled from outside the notebook\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "action_size = 1\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "observation_size = len(state)\n",
    "\n",
    "# Initialize the policy and target networks\n",
    "policy_net = DQN(observation_size, action_size).to(device)\n",
    "target_net = DQN(observation_size, action_size).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())  # copy weights\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True) \n",
    "\n",
    "# Initialize the replay memory\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# Initialize the noise process\n",
    "seed = random.seed(42)\n",
    "noise = OUNoise(1, seed, theta=0.15, sigma=0.2) \n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    \"\"\"Select an action from the policy network with added noise from the noise process.\"\"\"\n",
    "    global steps_done\n",
    "\n",
    "    steps_done += 1\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # NEW: added .view(1, 1)\n",
    "        return policy_net(state)[2].view(1, 1) + noise.sample() # QUESTION: what does view do?\n",
    "\n",
    "# track the duration of each episode aand the scores\n",
    "episode_durations = []\n",
    "episode_scores = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(show_result=False):\n",
    "    plt.figure(1)\n",
    "    scores_t = torch.tensor(episode_scores, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.plot(scores_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(scores_t) >= 100:\n",
    "        means = scores_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # sample transitions\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    \n",
    "    state_action_values = policy_net(state_batch, action_batch)[0]\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states)[1].reshape(-1)\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch \n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up diagnostics\n",
    "naf_diagnostics = {'rewards': [],\n",
    "               'actions': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 400\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    # Initialize the environment and get it's state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "    \n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            episode_scores.append(sum(rewards))\n",
    "            plot_scores()\n",
    "            noise.reset()\n",
    "            break\n",
    "    noise.reset()\n",
    "\n",
    "    naf_diagnostics['rewards'].append(rewards)\n",
    "    naf_diagnostics['actions'].append(actions)\n",
    "\n",
    "\n",
    "print('Complete')\n",
    "plot_scores(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
