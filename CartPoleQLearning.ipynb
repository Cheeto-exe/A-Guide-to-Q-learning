{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: https://medium.com/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5\n",
    "         https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Space : Discrete(2)\n",
    "# Observation Shape : (4,)\n",
    "# Observation High : [4.8 inf 0.42 inf]\n",
    "# Observation Low : [-4.8 -inf -0.42 -inf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Space: 0 - go left, 1  - go right\n",
    "\n",
    "# Observation[0]: Cart Position in [-4.8, 4.8]\n",
    "# Observation[1]: Cart Velocity in [-inf, inf]\n",
    "# Observation[2]: Pole Angle in [-0.42, 0.42]\n",
    "# Observation[3]: Pole Angular Velocity [-inf, inf]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinearModel:\n",
    "    \"\"\"Q function approximator using linear model.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, n_actions, feature_transformer):\n",
    "        \"\"\"Initialize the model.\"\"\"\n",
    "        \n",
    "        self.weights = np.random.uniform(-1e-3,1e-3,[n_features, n_actions])\n",
    "        self.feature_transformer = feature_transformer\n",
    "        \n",
    "    def approx(self, obs, action):\n",
    "        \"\"\"Approximate the Q value for a given state and action.\"\"\"\n",
    "        \n",
    "        return self.weights[:, action].dot(self.feature_transformer.transform(obs))\n",
    "    \n",
    "    def update(self, delta, action):\n",
    "        \"\"\"Update the weights of the model.\"\"\"\n",
    "        \n",
    "        self.weights[:, action] = self.weights[:, action] + delta\n",
    "        \n",
    "    def max_Q(self, obs):\n",
    "        \"\"\"Return the action with the highest Q value.\"\"\"\n",
    "        \n",
    "        q_values = self.weights.T.dot(self.feature_transformer.transform(obs))\n",
    "        return np.argmax(q_values), np.max(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddInterceptFeatures:\n",
    "    \"\"\"Add an intercept.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, obs):\n",
    "        \"\"\"Add an intercept term to an observation.\"\"\"\n",
    "        return np.concatenate([obs, np.ones((1,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFFeatures:\n",
    "    \"\"\"Radial basis function features.\"\"\"\n",
    "\n",
    "    def __init__(self, n_features, centers, widths):\n",
    "        \"\"\"Initialize the model.\"\"\"\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.centers = centers\n",
    "        self.widths = widths\n",
    "\n",
    "    def train(self, X):\n",
    "        \"\"\"Obtain the centers and widths of the RBFs.\"\"\"\n",
    "        \n",
    "        self.centers = np.mean(X, axis=0)\n",
    "        self.widths = np.std(X, axis=0)\n",
    "\n",
    "    def transform(self, obs):\n",
    "        \"\"\"Transform an observation into RBF features.\"\"\"\n",
    "\n",
    "        return np.exp(-0.5 * np.sum((obs.reshape(-1,1) - self.centers)**2 / self.widths**2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('observation', 'action', 'next_observation', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"Experience replay memory.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"Initialize the memory.\"\"\"\n",
    "\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions.\"\"\"\n",
    "        \n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the memory.\"\"\"\n",
    "        \n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-sparse reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(obs):\n",
    "    \"\"\"Non-sparse reward function for the cartpole problem.\"\"\"\n",
    "\n",
    "    return 1 - obs[2]**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \"\"\"Q-learning algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, n_features, n_actions, feature_transformer):\n",
    "        \"\"\"Initialize the algorithm.\"\"\"\n",
    "\n",
    "        # self.env = env\n",
    "        self.feature_transformer = feature_transformer\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "\n",
    "    def train(self, reward_func = None, episodes=5000, penalty=0.95, learning_rate=0.001, timestep=100, epsilon_decay=0.999, regul_strength=1e-4):\n",
    "        \"\"\"Train the Q-function approximator.\"\"\"\n",
    "\n",
    "        # set parameters\n",
    "        epsilon = 1     # exploration rate\n",
    "        lmbda = regul_strength    # l2 regularisation\n",
    "\n",
    "\n",
    "        # initialise the Q function approximation model\n",
    "        self.qlm = QLinearModel(n_features=self.n_features, n_actions=self.n_actions, feature_transformer=self.feature_transformer)\n",
    "\n",
    "\n",
    "        # reset counters\n",
    "        rewards = 0\n",
    "        steps = 0 \n",
    "        self.runs = [0]\n",
    "        self.avgs = [0]\n",
    "\n",
    "        # solved flag\n",
    "        solved = False\n",
    "\n",
    "        # initialise diagnostics\n",
    "        self.diagnostics = {'actions': [], 'rewards': [], 'max_Q_for_new': [], 'current_Q': [], 'TD_error': [], 'delta_weights': []}\n",
    "\n",
    "\n",
    "        for episode in range(1, episodes+1):\n",
    "\n",
    "            # decay epsilon\n",
    "            epsilon = epsilon * epsilon_decay\n",
    "            \n",
    "            # initialise the environment and obtain the observation\n",
    "            current_observation = env.reset()[0] \n",
    "            \n",
    "            # reset score and terminated flag\n",
    "            score = 0\n",
    "            terminated = False\n",
    "            \n",
    "            while not terminated:\n",
    "                steps += 1\n",
    "                \n",
    "        #         if episode % timestep == 0:\n",
    "        #             env.render()\n",
    "                \n",
    "                # epsilon-greedy action selection\n",
    "                if np.random.uniform(0,1) < epsilon:\n",
    "                    action = env.action_space.sample() # pick a random action\n",
    "                else:\n",
    "                    action = self.qlm.max_Q(current_observation)[0] # pick the best action for a given state\n",
    "                \n",
    "                # perform action and obtain reward and a new observation\n",
    "                next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "                # augment reward\n",
    "                if reward_func is not None:\n",
    "                    reward = reward_func(next_observation)\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                # store transition in replay memory\n",
    "                memory.push(current_observation, action, next_observation, reward)\n",
    "\n",
    "                if not terminated:\n",
    "                    \n",
    "                    # calculate TD error\n",
    "                    max_Q_for_new_obs = self.qlm.max_Q(next_observation)[1]\n",
    "                    current_Q = self.qlm.approx(current_observation, action)\n",
    "                    delta_weights = learning_rate \\\n",
    "                                    * (reward + penalty * max_Q_for_new_obs - current_Q) \\\n",
    "                                    * self.qlm.feature_transformer.transform(current_observation)\n",
    "                    \n",
    "                    # add l2 regularisation to delta_weights\n",
    "                    delta_weights = delta_weights - lmbda * self.qlm.weights[:, action]\n",
    "\n",
    "                    # update weights\n",
    "                    self.qlm.update(delta_weights, action)\n",
    "\n",
    "                    # append diagnostics\n",
    "                    self.diagnostics['actions'].append(action)\n",
    "                    self.diagnostics['rewards'].append(reward)\n",
    "                    self.diagnostics['max_Q_for_new'].append(max_Q_for_new_obs)\n",
    "                    self.diagnostics['current_Q'].append(current_Q)\n",
    "                    self.diagnostics['TD_error'].append(reward + penalty * max_Q_for_new_obs - current_Q)\n",
    "                    self.diagnostics['delta_weights'].append(delta_weights)\n",
    "                    \n",
    "                # update current observation\n",
    "                current_observation = next_observation\n",
    "                \n",
    "            else:\n",
    "                # append score\n",
    "                rewards += score\n",
    "                self.runs.append(score)\n",
    "                \n",
    "                # check if solved\n",
    "                if score > 195 and steps >= 100 and solved == False:\n",
    "                    solved = True\n",
    "                    print('Solved')\n",
    "                    \n",
    "            if episode%timestep == 0:\n",
    "                    self.avgs.append(rewards/timestep)\n",
    "                    rewards = 0\n",
    "      \n",
    "        env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning with Experience Replay (we did not observe convergence for this implementation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full Q-learning with replay buffer (from CS285):\n",
    "1. collect dataset $\\left\\{\\left(\\mathbf{s}_i, \\mathbf{a}_i, \\mathbf{s}_i^{\\prime}, r_i\\right)\\right\\}$ using some policy, add it to $\\mathcal{B}$\n",
    "Repeat K times:\n",
    "- 2. sample a batch $\\left(\\mathbf{s}_i, \\mathbf{a}_i, \\mathbf{s}_i^{\\prime}, r_i\\right)$ from $\\mathcal{B}$\n",
    "- 3. $\\phi \\leftarrow \\phi-\\alpha \\sum_i \\frac{d Q_\\phi}{d \\phi}\\left(\\mathbf{s}_i, \\mathbf{a}_i\\right)\\left(Q_\\phi\\left(\\mathbf{s}_i, \\mathbf{a}_i\\right)-\\left[r\\left(\\mathbf{s}_i, \\mathbf{a}_i\\right)+\\gamma \\max _{\\mathbf{a}^{\\prime}} Q_\\phi\\left(\\mathbf{s}_i^{\\prime}, \\mathbf{a}_i^{\\prime}\\right)\\right]\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningWithExperienceReplay:\n",
    "    \"\"\"Q-learning algorithm with experience replay.\"\"\"\n",
    "\n",
    "    def __init__(self, n_features, n_actions, feature_transformer):\n",
    "        \"\"\"Initialize the algorithm.\"\"\"\n",
    "\n",
    "        # self.env = env\n",
    "        self.feature_transformer = feature_transformer\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "         # initialise the Q function approximation model\n",
    "        self.qlm = QLinearModel(n_features=self.n_features, n_actions=self.n_actions, feature_transformer=self.feature_transformer)\n",
    "\n",
    "\n",
    "    def train(self, K, batch_size, learning_rate, reward_func=None, penalty=0.95):\n",
    "        \"\"\"Train the Q-function approximator using stochastic gradient descent with mini-batches.\"\"\"\n",
    "\n",
    "        # initialise the Q function approximation model\n",
    "        self.qlm = QLinearModel(n_features=self.n_features, n_actions=self.n_actions, feature_transformer=self.feature_transformer)\n",
    "\n",
    "        # peform K iterations of experience replay\n",
    "        for _ in range(K):\n",
    "\n",
    "            # sample a batch of transitions\n",
    "            batch = memory.sample(batch_size)\n",
    "\n",
    "\n",
    "            for i in range(batch_size):\n",
    "\n",
    "                batch_delta_weights = {0: [], 1: []}\n",
    "\n",
    "                # extract the transition\n",
    "                current_observation, action, next_observation, reward = batch[i]\n",
    "\n",
    "                # augment reward\n",
    "                if reward_func is not None:\n",
    "                    reward = reward_func(next_observation)\n",
    "\n",
    "                # calculate TD error\n",
    "                max_Q_for_new_obs = self.qlm.max_Q(next_observation)[1]\n",
    "                current_Q = self.qlm.approx(current_observation, action)\n",
    "                delta_weights = (reward + penalty * max_Q_for_new_obs - current_Q) \\\n",
    "                                * self.qlm.feature_transformer.transform(current_observation)\n",
    "\n",
    "                batch_delta_weights[action].append(delta_weights)\n",
    "\n",
    "            # average the delta weights\n",
    "            mean_delta_weights = {0: np.mean(batch_delta_weights[0], axis=0) if len(batch_delta_weights[0])>0 else 0, \n",
    "                                  1: np.mean(batch_delta_weights[1], axis=0) if len(batch_delta_weights[1])>0 else 0}\n",
    "\n",
    "            # update weights\n",
    "            self.qlm.update(learning_rate * mean_delta_weights[0], 0)\n",
    "            self.qlm.update(learning_rate * mean_delta_weights[1], 1)\n",
    "\n",
    "    def run(self, reward_func=None, episodes=100, epsilon=0):\n",
    "        \"\"\"Run the algorithm for a given number of episodes.\"\"\"\n",
    "\n",
    "        # reset counters\n",
    "        rewards = 0\n",
    "        steps = 0\n",
    "        runs = []\n",
    "\n",
    "        # initialise diagnostics\n",
    "        self.diagnostics = {'actions': [], 'rewards': [], 'max_Q_for_new': [], 'current_Q': [], 'TD_error': [], 'delta_weights': []}\n",
    "\n",
    "        # solved flag\n",
    "        solved = False\n",
    "\n",
    "        for episode in range(1, episodes+1):\n",
    "            \n",
    "            # initialise the environment and obtain the observation\n",
    "            current_observation = env.reset()[0] \n",
    "            \n",
    "            score = 0\n",
    "            terminated = False\n",
    "\n",
    "            # epsilon = epsilon * epsilon_decay\n",
    "            \n",
    "            while not terminated:\n",
    "                steps += 1\n",
    "                \n",
    "                # epsilon-greedy policy\n",
    "                if np.random.uniform(0,1) < epsilon:\n",
    "                    action = env.action_space.sample() # pick a random action\n",
    "                else:\n",
    "                    action = self.qlm.max_Q(current_observation)[0] # pick the best action for a given state\n",
    "                    \n",
    "                # perform action and obtain reward and a new observation\n",
    "                next_observation, reward, terminated, truncated, info = env.step(action) \n",
    "                score += reward\n",
    "\n",
    "                # augment reward\n",
    "                if reward_func is not None:\n",
    "                    reward = reward_func(next_observation)\n",
    "\n",
    "\n",
    "        \n",
    "                # store transition in replay memory\n",
    "                memory.push(current_observation, action, next_observation, reward)\n",
    "                    \n",
    "                current_observation = next_observation\n",
    "                \n",
    "            else:\n",
    "                # append score\n",
    "                rewards += score\n",
    "                runs.append(score)\n",
    "                \n",
    "                # check if solved\n",
    "                if score > 195 and steps >= 100 and solved == False: \n",
    "                    solved = True\n",
    "                    print('Solved')\n",
    "        \n",
    "                           \n",
    "        env.close()\n",
    "        return runs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning with experience replay (Combined Q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Zhang S, Sutton R. A Deeper Look at Experience Replay [Internet]. Available from: https://arxiv.org/pdf/1712.01275.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedQ:\n",
    "\n",
    "    def __init__(self, n_features, n_actions, feature_transformer):\n",
    "        \"\"\"Initialize the algorithm.\"\"\"\n",
    "\n",
    "        # self.env = env\n",
    "        self.feature_transformer = feature_transformer\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # initialise diagnostics\n",
    "        self.diagnostics = {'actions': [], 'rewards': [], 'max_Q_for_new': [], 'current_Q': [], 'TD_error': [], 'delta_weights': []}\n",
    "\n",
    "        # initialise the Q function approximation model\n",
    "        self.qlm = QLinearModel(n_features=self.n_features, n_actions=self.n_actions, feature_transformer=self.feature_transformer)\n",
    "\n",
    "\n",
    "    def train(self, K, batch_size, learning_rate, reward_func=None, episodes=100, epsilon_decay=0.999, penalty=0.95):\n",
    "        \"\"\"Run the algorithm for a given number of episodes.\"\"\"\n",
    "\n",
    "        # reset counters\n",
    "        rewards = 0\n",
    "        steps = 0\n",
    "        self.runs = []\n",
    "\n",
    "        epsilon = 1\n",
    "\n",
    "\n",
    "\n",
    "        # solved flag\n",
    "        solved = False\n",
    "\n",
    "        for episode in range(1, episodes+1):\n",
    "            # print(episode)\n",
    "            \n",
    "            # initialise the environment and obtain the observation\n",
    "            current_observation = env.reset()[0] \n",
    "            \n",
    "            score = 0\n",
    "            terminated = False\n",
    "\n",
    "            epsilon = epsilon * epsilon_decay\n",
    "            \n",
    "            while not terminated:\n",
    "                steps += 1\n",
    "                \n",
    "                # epsilon-greedy policy\n",
    "                if np.random.uniform(0,1) < epsilon:\n",
    "                    action = env.action_space.sample() # pick a random action\n",
    "                else:\n",
    "                    action = self.qlm.max_Q(current_observation)[0] # pick the best action for a given state\n",
    "                    \n",
    "                # perform action and obtain reward and a new observation\n",
    "                next_observation, reward, terminated, truncated, info = env.step(action) \n",
    "                \n",
    "                # augment reward\n",
    "                if reward_func is not None:\n",
    "                    reward = reward_func(next_observation)\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                # create transition\n",
    "                transition_t = Transition(current_observation, action, next_observation, reward)\n",
    "\n",
    "                # store transition in replay memory\n",
    "                memory.push(current_observation, action, next_observation, reward)\n",
    "\n",
    "                if not terminated:\n",
    "                    # sample a batch of transitions from the replay memory and update the Q function approximation model\n",
    "\n",
    "                    # peform K iterations of experience replay\n",
    "                    for _ in range(K):\n",
    "\n",
    "                        # sample a batch of transitions\n",
    "                        batch = memory.sample(batch_size)\n",
    "                        batch.append(transition_t)\n",
    "\n",
    "\n",
    "                        for i in range(len(batch)):\n",
    "\n",
    "                            # extract the transition\n",
    "                            sample_current_observation, sample_action, sample_next_observation, sample_reward = batch[i]\n",
    "\n",
    "                            # augment reward\n",
    "                            if reward_func is not None:\n",
    "                                reward = reward_func(sample_next_observation)\n",
    "\n",
    "                            # calculate TD error\n",
    "                            max_Q_for_new_obs = self.qlm.max_Q(sample_next_observation)[1]\n",
    "                            current_Q = self.qlm.approx(sample_current_observation, sample_action)\n",
    "                            delta_weights = learning_rate * (reward + penalty * max_Q_for_new_obs - current_Q) \\\n",
    "                                            * self.qlm.feature_transformer.transform(sample_current_observation)\n",
    "\n",
    "                            delta_weights = delta_weights - 1e-3 * self.qlm.weights[:, sample_action]\n",
    "\n",
    "                            self.qlm.update(delta_weights, sample_action)\n",
    "\n",
    "                             # append diagnostics\n",
    "                            self.diagnostics['actions'].append(sample_action)\n",
    "                            self.diagnostics['rewards'].append(sample_reward)\n",
    "                            self.diagnostics['max_Q_for_new'].append(max_Q_for_new_obs)\n",
    "                            self.diagnostics['current_Q'].append(current_Q)\n",
    "                            self.diagnostics['TD_error'].append(sample_reward + penalty * max_Q_for_new_obs - current_Q)\n",
    "                            self.diagnostics['delta_weights'].append(delta_weights)\n",
    "\n",
    "                # S <- S'\n",
    "                current_observation = next_observation\n",
    "                \n",
    "            else:\n",
    "                # append score\n",
    "                rewards += score\n",
    "                self.runs.append(score)\n",
    "                \n",
    "                # check if solved\n",
    "                if score > 195 and steps >= 100 and solved == False: \n",
    "                    solved = True\n",
    "                    print('Solved')\n",
    "        \n",
    "                        \n",
    "        env.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
