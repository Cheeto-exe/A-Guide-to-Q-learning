{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the definitions included in the file\n",
    "%run CartPoleQLearning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the memory\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the font size in all matplotlib plots\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# increase the title font size in all matplotlib plots\n",
    "plt.rcParams.update({'axes.titlesize':22})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole with Q-learning with Linear Q-function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_basic = QLearning(n_features=5, n_actions=2, feature_transformer=AddInterceptFeatures())\n",
    "ql_basic.train(episodes=1000, regul_strength=1e-2, learning_rate=7e-6,  epsilon_decay=0.995, reward_func=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average reward over the last 100 episodes\n",
    "avgs = [0 for _ in range(100)]\n",
    "for i in range(100, len(ql_basic.runs)+1):\n",
    "    avgs.append(np.mean(ql_basic.runs[i-100:i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reward per episode and the average reward per 100 episodes\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.plot(ql_basic.runs, label='Reward per episode')\n",
    "ax.plot(avgs, label='Average reward per 100 episodes')\n",
    "ax.lines[1].set_linewidth(3)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Q-learning with linear function approximation')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the figure as pdf\n",
    "fig.savefig('./Plots/cartpole_q_learning.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {}\n",
    "for lr in 10**np.linspace(-7, -2, 20, dtype=float):\n",
    "    for regul_strength in 10**np.arange(-4,0,0.5, dtype=float):\n",
    "        print(lr, regul_strength)\n",
    "        grid_search[(lr, regul_strength)] = []\n",
    "        for i in range(10):\n",
    "            ql_basic = QLearning(n_features=5, n_actions=2, feature_transformer=AddInterceptFeatures())\n",
    "            ql_basic.train(episodes=5000, regul_strength=regul_strength, learning_rate=lr, reward_func=None)\n",
    "            grid_search[(lr, regul_strength)].append(ql_basic.avgs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert jsonifiable format\n",
    "grid_search_json = {}\n",
    "for lr, regul_strength in grid_search.keys():\n",
    "    grid_search_json[lr]= {}\n",
    "for lr, regul_strength in grid_search.keys():\n",
    "    grid_search_json[lr][regul_strength] = grid_search[(lr, regul_strength)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "with open('grid_search.json', 'w') as fp:\n",
    "    json.dump(grid_search_json, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load from json\n",
    "with open('grid_search.json', 'r') as fp:\n",
    "    grid_search_json = json.load(fp)\n",
    "\n",
    "grid_search = {}\n",
    "for lr in grid_search_json.keys():\n",
    "    for regul_strength in grid_search_json[lr].keys():\n",
    "        grid_search[(float(lr), float(regul_strength))] = grid_search_json[lr][regul_strength]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_maxs = {}\n",
    "grid_search_prop_success = {}\n",
    "grid_search_avgs = {}\n",
    "for lr_regul in grid_search.keys():\n",
    "    avgs = grid_search[lr_regul]\n",
    "    grid_search_maxs[lr_regul] = max([avg[-1] for avg in avgs])\n",
    "    grid_search_prop_success[lr_regul] = sum([1 for avg in avgs if avg[-1]>40])/len(avgs)\n",
    "    grid_search_avgs[lr_regul] = np.mean([avg[-1] for avg in avgs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(3,1, figsize=(10, 20))\n",
    "im0 = ax[0].imshow(np.array(list(grid_search_maxs.values())).reshape(20,8).T, cmap='magma', interpolation='nearest')\n",
    "ax[0].set_yticks(np.arange(8))\n",
    "ax[0].set_yticklabels([f'{regul_strength:.2f}' for regul_strength in 10**np.arange(-4,0,0.5, dtype=float)])\n",
    "ax[0].set_xticks(np.arange(20))\n",
    "ax[0].set_xticklabels([f'{lr:.2e}' for lr in 10**np.linspace(-7, -2, 20, dtype=float)])\n",
    "plt.setp(ax[0].get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "ax[0].set_ylabel('Regularisation strength')\n",
    "ax[0].set_xlabel('Learning rate')\n",
    "ax[0].set_title('Maximum final average reward over 10 initializations')\n",
    "fig.colorbar(im0, ax=ax[0])\n",
    "\n",
    "\n",
    "im1 = ax[1].imshow(np.array(list(grid_search_prop_success.values())).reshape(20,8).T, cmap='magma', interpolation='nearest')\n",
    "ax[1].set_yticks(np.arange(8))\n",
    "ax[1].set_yticklabels([f'{regul_strength:.2f}' for regul_strength in 10**np.arange(-4,0,0.5, dtype=float)])\n",
    "ax[1].set_xticks(np.arange(20))\n",
    "ax[1].set_xticklabels([f'{lr:.2e}' for lr in 10**np.linspace(-7, -2, 20, dtype=float)])\n",
    "plt.setp(ax[1].get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "ax[1].set_ylabel('Regularisation strength')\n",
    "ax[1].set_xlabel('Learning rate')\n",
    "ax[1].set_title('Proportion of final averages >40 per 10 initializations')\n",
    "fig.colorbar(im1, ax=ax[1])\n",
    "\n",
    "im1 = ax[2].imshow(np.array(list(grid_search_avgs.values())).reshape(20,8).T, cmap='magma', interpolation='nearest')\n",
    "ax[2].set_yticks(np.arange(8))\n",
    "ax[2].set_yticklabels([f'{regul_strength:.3f}' for regul_strength in 10**np.arange(-4,0,0.5, dtype=float)])\n",
    "ax[2].set_xticks(np.arange(20))\n",
    "ax[2].set_xticklabels([f'{lr:.2e}' for lr in 10**np.linspace(-7, -2, 20, dtype=float)])\n",
    "plt.setp(ax[2].get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "ax[2].set_ylabel('Regularisation strength')\n",
    "ax[2].set_xlabel('Learning rate')\n",
    "ax[2].set_title('Average of final average rewards per 10 initializations')\n",
    "fig.colorbar(im1, ax=ax[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the figure as pdf\n",
    "fig.savefig('./Plots/cartpole_grid_search.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(15, 6))\n",
    "im0 = ax.imshow(np.array(list(grid_search_maxs.values())).reshape(20,8).T, cmap='magma', interpolation='nearest')\n",
    "ax.set_yticks(np.arange(8))\n",
    "ax.set_yticklabels([f'1e{regul_strength:.1f}' for regul_strength in np.arange(-4,0,0.5, dtype=float)])\n",
    "ax.set_xticks(np.arange(20))\n",
    "ax.set_xticklabels([f'1e{lr:.2f}' for lr in np.linspace(-7, -2, 20, dtype=float)])\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "ax.set_ylabel('Regularisation strength')\n",
    "ax.set_xlabel('Learning rate')\n",
    "ax.set_title('Maximum final average reward over 10 initializations')\n",
    "fig.colorbar(im0, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the figure as pdf\n",
    "fig.savefig('./Plots/cartpole_grid_search_maxs.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(15, 6))\n",
    " \n",
    "im1 = ax.imshow(np.array(list(grid_search_prop_success.values())).reshape(20,8).T, cmap='magma', interpolation='nearest')\n",
    "ax.set_yticks(np.arange(8))\n",
    "ax.set_yticklabels([f'1e{regul_strength:.1f}' for regul_strength in np.arange(-4,0,0.5, dtype=float)])\n",
    "ax.set_xticks(np.arange(20))\n",
    "ax.set_xticklabels([f'1e{lr:.2f}' for lr in np.linspace(-7, -2, 20, dtype=float)])\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "ax.set_ylabel('Regularisation strength')\n",
    "ax.set_xlabel('Learning rate')\n",
    "ax.set_title(r'Proportion of successful initializations')\n",
    "fig.colorbar(im1, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the figure as pdf\n",
    "fig.savefig('./Plots/cartpole_grid_search_props.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 5 max lr regul pairs for grid_search_maxs\n",
    "top5 = sorted(grid_search_maxs, key=grid_search_maxs.get, reverse=True)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[grid_search_maxs[el] for el in top5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning with augmented reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_basic_aug = QLearning(n_features=5, n_actions=2, feature_transformer=AddInterceptFeatures())\n",
    "ql_basic_aug.train(episodes=1000, regul_strength=1e-2, learning_rate=7e-6,  epsilon_decay=0.995, reward_func=reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average reward over the last 100 episodes\n",
    "avgs_aug = [0 for _ in range(100)]\n",
    "for i in range(100, len(ql_basic_aug.runs)+1):\n",
    "    avgs_aug.append(np.mean(ql_basic_aug.runs[i-100:i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reward per episode and the average reward per 100 episodes\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.plot(ql_basic_aug.runs, label='Reward per episode')\n",
    "ax.plot(avgs_aug, label='Average reward per 100 episodes')\n",
    "\n",
    "ax.lines[1].set_linewidth(3)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Q-learning with linear function approximation and augmented rewards')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the figure as pdf\n",
    "fig.savefig('./Plots/cartpole_q_learning_augmented_reward.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the comparison \n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.plot(avgs, label='Average reward per 100 episodes (default rewards)')\n",
    "ax.plot(avgs_aug, label='Average reward per 100 episodes (augmented rewards)')\n",
    "ax.lines[1].set_color('purple')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Impact of augmented rewards on Q-learning with linear function approximation')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the figure as pdf\n",
    "fig.savefig('./Plots/cartpole_q_learning_augmented_reward_comparison.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning with Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the memory\n",
    "qler = QLearningWithExperienceReplay(n_features=5, n_actions=2, feature_transformer=AddInterceptFeatures())\n",
    "qler.run(episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cer = CombinedQ(n_features=5, n_actions=2, feature_transformer=AddInterceptFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cer.train(K=1, batch_size=128, learning_rate=1e-4, reward_func=None, episodes=1000, epsilon_decay=0.995, penalty=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average reward over the last 100 episodes\n",
    "avgs = [0 for _ in range(100)]\n",
    "for i in range(100, len(cer.runs)+1):\n",
    "    avgs.append(np.mean(cer.runs[i-100:i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reward per episode and the average reward per 100 episodes\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.plot(cer.runs, label='Reward per episode')\n",
    "ax.plot(avgs, label='Average reward per 100 episodes')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "\n",
    "ax.lines[1].set_linewidth(3)\n",
    "ax.set_title('Q-learning with experience replay')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save to pdf\n",
    "fig.savefig('./Plots/combined_q_learning_with_experience_replay.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cer = {}\n",
    "for lr in 10**np.arange(-7,-1, dtype=float):\n",
    "    grid_search_cer[lr] = {}\n",
    "    for K in [1, 5, 10]:\n",
    "        grid_search_cer[lr][K] = {}\n",
    "        for batch_size in [8, 32, 128, 256]:\n",
    "            grid_search_cer[lr][K][batch_size] = {}\n",
    "            print(lr, K, batch_size)\n",
    "            cer = CombinedQ(n_features=5, n_actions=2, feature_transformer=AddInterceptFeatures())\n",
    "            cer.run(K=K, batch_size=batch_size, learning_rate=lr, reward_func=None, episodes=400, epsilon_decay=0.995, penalty=0.99)\n",
    "            grid_search_cer[lr][K][batch_size] = cer.runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cer_buffer = {}\n",
    "\n",
    "for buffer_size in [1000, 10000, 100000, 1000000, 10000000]:\n",
    "    print(buffer_size)\n",
    "\n",
    "    memory = ReplayMemory(buffer_size)\n",
    "\n",
    "    # feed the memory with random actions\n",
    "    qler = QLearningWithExperienceReplay(n_features=5, n_actions=2, feature_transformer=AddInterceptFeatures())\n",
    "    qler.run(episodes=20)\n",
    "\n",
    "    grid_search_cer_buffer[buffer_size] = []\n",
    "\n",
    "    for _ in range(5):\n",
    "        print(_)\n",
    "\n",
    "        cer = CombinedQ(n_features=5, n_actions=2, feature_transformer=AddInterceptFeatures())\n",
    "        cer.train(K=1, batch_size=32, learning_rate=1e-6, reward_func=None, episodes=1000, epsilon_decay=0.995, penalty=0.99)\n",
    "        grid_search_cer_buffer[buffer_size].append(cer.runs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to json\n",
    "with open('grid_search_cer_buffer.json', 'w') as fp:\n",
    "    json.dump(grid_search_cer_buffer, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average reward over the last 100 episodes\n",
    "grid_search_cer_buffer_last_avgs = {}\n",
    "for buffer_size in grid_search_cer_buffer.keys():\n",
    "    grid_search_cer_buffer_last_avgs[buffer_size] = [np.mean(el[-100:]) for el in grid_search_cer_buffer[buffer_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.arange(5)\n",
    "y_1 = [grid_search_cer_buffer_last_avgs[x][0] for x in grid_search_cer_buffer_last_avgs.keys()]\n",
    "y_2 = [grid_search_cer_buffer_last_avgs[x][1] for x in grid_search_cer_buffer_last_avgs.keys()]\n",
    "y_3 = [grid_search_cer_buffer_last_avgs[x][2] for x in grid_search_cer_buffer_last_avgs.keys()]\n",
    "y_4 = [grid_search_cer_buffer_last_avgs[x][3] for x in grid_search_cer_buffer_last_avgs.keys()]\n",
    "y_5 = [grid_search_cer_buffer_last_avgs[x][4] for x in grid_search_cer_buffer_last_avgs.keys()]\n",
    "\n",
    "width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.bar(x - 2*width, y_1, width, label='Run 1', color='cornflowerblue') \n",
    "ax.bar(x - width, y_2, width, label='Run 2', color='gold')\n",
    "ax.bar(x, y_3, width, label='Run 3', color ='seagreen')\n",
    "ax.bar(x + width, y_4, width, label='Run 4', color='coral')\n",
    "ax.bar(x + 2*width, y_5, width, label='Run 5', color='purple')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(grid_search_cer_buffer_last_avgs.keys())\n",
    "ax.set_xlabel('Buffer size')\n",
    "ax.set_ylabel('Average reward')\n",
    "ax.set_title('Average reward over last 100 episodes for different buffer sizes')\n",
    "ax.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pdf\n",
    "fig.savefig('./Plots/combined_q_learning_with_experience_replay_buffer_size.pdf', bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole with Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "%run CartPoleDQN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average reward over the last 100 episodes\n",
    "dqn_avgs = [0 for _ in range(100)]\n",
    "for i in range(100, len(episode_durations)+1):\n",
    "    dqn_avgs.append(np.mean(episode_durations[i-100:i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reward per episode and the average reward per 100 episodes\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.plot(episode_durations, label='Reward per epsisode')\n",
    "ax.plot(dqn_avgs, label='Average reward per 100 episodes')\n",
    "\n",
    "ax.lines[1].set_linewidth(3)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Deep Q-Network')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the figure as pdf\n",
    "fig.savefig('./Plots/cartpole_dqn.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_DQN = {}\n",
    "\n",
    "for LR in 10**np.arange(-8, -1, dtype=float):\n",
    "    print(LR)\n",
    "    gridsearch_DQN[LR] = {}\n",
    "    gridsearch_DQN[LR]['episode_durations'] = []\n",
    "    gridsearch_DQN[LR]['averages'] = []\n",
    "    for i in range(5):\n",
    "        %run CartPoleDQN.ipynb\n",
    "        gridsearch_DQN[LR]['episode_durations'].append(durations_t)\n",
    "        gridsearch_DQN[LR]['averages'].append(means)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_averages = {}\n",
    "for lr in gridsearch_DQN.keys():\n",
    "    final_averages[lr] = [el[-1].item() for el in gridsearch_DQN[lr]['averages']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_averages_max = {}\n",
    "final_averages_avg = {}\n",
    "for lr in final_averages.keys():\n",
    "    final_averages_max[lr] = max(final_averages[lr])\n",
    "    final_averages_avg[lr] = np.mean(final_averages[lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_actions = [el.item() for el in diagnostics['actions'][-1]]\n",
    "dqn_rewards = [el.item() for el in diagnostics['rewards'][-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the actions taken by the DQN\n",
    "fig, ax = plt.subplots(figsize=(15,3))\n",
    "ax.plot(dqn_actions[0:100], label= 'DQN action', c='green' , marker='o')\n",
    "\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Action')\n",
    "ax.set_title('DQN actions')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the figure as pdf\n",
    "fig.savefig('./Plots/cartpole_dqn_actions.pdf', bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car with Normalized Advantage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "%run MountainCarDQN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naf_scores = [score.item() for score in episode_scores]\n",
    "\n",
    "# calculate the average reward over the last 100 episodes\n",
    "naf_averages = [0 for _ in range(100)]\n",
    "for i in range(100, len(naf_scores)):\n",
    "    naf_averages.append(sum(naf_scores[i-100:i])/100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reward per episode and the average reward per 100 episodes\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.plot(naf_scores, label= 'Reward per episode')\n",
    "ax.plot(naf_averages, label='Average reward per 100 episodes')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "\n",
    "ax.lines[1].set_linewidth(3)\n",
    "ax.set_title('Normalized Advantage Function')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the figure as pdf\n",
    "fig.savefig('./Plots/mountaincar_NAF.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naf_actions = [el.item() for el in naf_diagnostics['actions'][-4]]\n",
    "naf_rewards = [el.item() for el in naf_diagnostics['rewards'][-4]]\n",
    "naf_neg_rewards = [-el for el in naf_rewards]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the actions taken by the NAF and the penalty\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.plot(naf_actions, label= 'NAF action', c='green')\n",
    "ax.plot(naf_neg_rewards[:-1], label='NAF penalty', c='red')\n",
    "\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Action/Penalty')\n",
    "ax.set_title('NAF Actions and penalty')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the figure as pdf\n",
    "fig.savefig('./Plots/mountaincar_NAF_actions_penalty1.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naf_actions = [el.item() for el in naf_diagnostics['actions'][-1]]\n",
    "naf_rewards = [el.item() for el in naf_diagnostics['rewards'][-1]]\n",
    "naf_neg_rewards = [-el for el in naf_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the actions taken by the NAF and the penalty\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.plot(naf_actions, label= 'NAF action', c='green')\n",
    "ax.plot(naf_neg_rewards[:-1], label='NAF penalty', c='red')\n",
    "\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Action/Penalty')\n",
    "ax.set_title('NAF Actions and penalty')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('./Plots/mountaincar_NAF_actions_penalty2.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for NAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_NAF = {}\n",
    "\n",
    "for lr in 10**np.arange(-8, -1, dtype=float):\n",
    "    print(LR)\n",
    "    gridsearch_NAF[LR] = {}\n",
    "    gridsearch_NAF[LR]['episode_durations'] = []\n",
    "    gridsearch_NAF[LR]['averages'] = []\n",
    "    for i in range(5):\n",
    "        %run MountainCarDQN.ipynb\n",
    "        gridsearch_NAF[LR]['episode_durations'].append(durations_t)\n",
    "        gridsearch_NAF[LR]['averages'].append(means)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
